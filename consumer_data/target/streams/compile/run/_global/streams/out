[0m[[0m[31merror[0m] [0m[0morg.apache.spark.sql.streaming.StreamingQueryException: Partition report_data-0's offset was changed from 26180 to 0, some data may have been missed. [0m
[0m[[0m[31merror[0m] [0m[0mSome data may have been lost because they are not available in Kafka any more; either the[0m
[0m[[0m[31merror[0m] [0m[0m data was aged out by Kafka or the topic may have been deleted before all the data in the[0m
[0m[[0m[31merror[0m] [0m[0m topic was processed. If you don't want your streaming query to fail on such cases, set the[0m
[0m[[0m[31merror[0m] [0m[0m source option "failOnDataLoss" to "false".[0m
[0m[[0m[31merror[0m] [0m[0m    [0m
[0m[[0m[31merror[0m] [0m[0m=== Streaming Query ===[0m
[0m[[0m[31merror[0m] [0m[0mIdentifier: [id = 7755a028-7366-40bf-91b5-7b69d5fffc98, runId = a60c8de0-55d0-45d1-8f36-595874ec1cf6][0m
[0m[[0m[31merror[0m] [0m[0mCurrent Committed Offsets: {KafkaV2[Subscribe[report_data]]: {"report_data":{"0":26180}}}[0m
[0m[[0m[31merror[0m] [0m[0mCurrent Available Offsets: {KafkaV2[Subscribe[report_data]]: {"report_data":{"0":0}}}[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mCurrent State: ACTIVE[0m
[0m[[0m[31merror[0m] [0m[0mThread State: RUNNABLE[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mLogical Plan:[0m
[0m[[0m[31merror[0m] [0m[0mProject [data#21.harmonywatcher_id AS harmonywatcher_id#23, data#21.current_location AS current_location#24, data#21.surrounding_citizens AS surrounding_citizens#25, data#21.words_heard AS words_heard#26, data#21.alert AS alert#27, data#21.timestamp AS timestamp#28][0m
[0m[[0m[31merror[0m] [0m[0m+- Project [from_json(StructField(harmonywatcher_id,StringType,false), StructField(current_location,StructType(StructField(latitude,DoubleType,false),StructField(longitude,DoubleType,false)),false), StructField(surrounding_citizens,ArrayType(StructType(StructField(name,StringType,false),StructField(harmonyscore,DoubleType,false)),true),false), StructField(words_heard,StringType,false), StructField(alert,BooleanType,false), StructField(timestamp,StringType,false), cast(value#8 as string), Some(Europe/Paris)) AS data#21][0m
[0m[[0m[31merror[0m] [0m[0m   +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@184321f0, KafkaV2[Subscribe[report_data]][0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:330)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: java.lang.IllegalStateException: Partition report_data-0's offset was changed from 26180 to 0, some data may have been missed. [0m
[0m[[0m[31merror[0m] [0m[0mSome data may have been lost because they are not available in Kafka any more; either the[0m
[0m[[0m[31merror[0m] [0m[0m data was aged out by Kafka or the topic may have been deleted before all the data in the[0m
[0m[[0m[31merror[0m] [0m[0m topic was processed. If you don't want your streaming query to fail on such cases, set the[0m
[0m[[0m[31merror[0m] [0m[0m source option "failOnDataLoss" to "false".[0m
[0m[[0m[31merror[0m] [0m[0m    [0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.reportDataLoss(KafkaMicroBatchStream.scala:313)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$planInputPartitions$1(KafkaMicroBatchStream.scala:200)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$planInputPartitions$1$adapted(KafkaMicroBatchStream.scala:200)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$getOffsetRangesFromResolvedOffsets$6(KafkaOffsetReaderConsumer.scala:535)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.immutable.List.map(List.scala:246)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.immutable.List.map(List.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getOffsetRangesFromResolvedOffsets(KafkaOffsetReaderConsumer.scala:530)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.planInputPartitions(KafkaMicroBatchStream.scala:200)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions$lzycompute(MicroBatchScanExec.scala:45)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions(MicroBatchScanExec.scala:45)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:142)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:141)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.supportsColumnar(MicroBatchScanExec.scala:29)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:153)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.IterableOnceOps.foldLeft(IterableOnce.scala:676)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.IterableOnceOps.foldLeft$(IterableOnce.scala:670)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1300)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:459)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:145)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:145)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:651)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:641)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:255)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrun[0m) org.apache.spark.sql.streaming.StreamingQueryException: Partition report_data-0's offset was changed from 26180 to 0, some data may have been missed. [0m
[0m[[0m[31merror[0m] [0m[0mSome data may have been lost because they are not available in Kafka any more; either the[0m
[0m[[0m[31merror[0m] [0m[0m data was aged out by Kafka or the topic may have been deleted before all the data in the[0m
[0m[[0m[31merror[0m] [0m[0m topic was processed. If you don't want your streaming query to fail on such cases, set the[0m
[0m[[0m[31merror[0m] [0m[0m source option "failOnDataLoss" to "false".[0m
[0m[[0m[31merror[0m] [0m[0m    [0m
[0m[[0m[31merror[0m] [0m[0m=== Streaming Query ===[0m
[0m[[0m[31merror[0m] [0m[0mIdentifier: [id = 7755a028-7366-40bf-91b5-7b69d5fffc98, runId = a60c8de0-55d0-45d1-8f36-595874ec1cf6][0m
[0m[[0m[31merror[0m] [0m[0mCurrent Committed Offsets: {KafkaV2[Subscribe[report_data]]: {"report_data":{"0":26180}}}[0m
[0m[[0m[31merror[0m] [0m[0mCurrent Available Offsets: {KafkaV2[Subscribe[report_data]]: {"report_data":{"0":0}}}[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mCurrent State: ACTIVE[0m
[0m[[0m[31merror[0m] [0m[0mThread State: RUNNABLE[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mLogical Plan:[0m
[0m[[0m[31merror[0m] [0m[0mProject [data#21.harmonywatcher_id AS harmonywatcher_id#23, data#21.current_location AS current_location#24, data#21.surrounding_citizens AS surrounding_citizens#25, data#21.words_heard AS words_heard#26, data#21.alert AS alert#27, data#21.timestamp AS timestamp#28][0m
[0m[[0m[31merror[0m] [0m[0m+- Project [from_json(StructField(harmonywatcher_id,StringType,false), StructField(current_location,StructType(StructField(latitude,DoubleType,false),StructField(longitude,DoubleType,false)),false), StructField(surrounding_citizens,ArrayType(StructType(StructField(name,StringType,false),StructField(harmonyscore,DoubleType,false)),true),false), StructField(words_heard,StringType,false), StructField(alert,BooleanType,false), StructField(timestamp,StringType,false), cast(value#8 as string), Some(Europe/Paris)) AS data#21][0m
[0m[[0m[31merror[0m] [0m[0m   +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@184321f0, KafkaV2[Subscribe[report_data]][0m
