[0m[[0m[31merror[0m] [0m[0morg.apache.spark.sql.streaming.StreamingQueryException: Column "surrounding_citizens_temp" not found in schema Some(StructType(StructField(id,IntegerType,false),StructField(harmonywatcher_id,StringType,false),StructField(current_location_latitude,DoubleType,false),StructField(current_location_longitude,DoubleType,false),StructField(words_heard,StringType,true),StructField(surrounding_citizens,StringType,true),StructField(alert,BooleanType,false),StructField(timestamp,TimestampType,false)))[0m
[0m[[0m[31merror[0m] [0m[0m=== Streaming Query ===[0m
[0m[[0m[31merror[0m] [0m[0mIdentifier: [id = 21651d0e-fe6c-489b-bd8b-d7941217915f, runId = 3ab10723-ba20-430b-a49a-2d91588b76f3][0m
[0m[[0m[31merror[0m] [0m[0mCurrent Committed Offsets: {}[0m
[0m[[0m[31merror[0m] [0m[0mCurrent Available Offsets: {KafkaV2[Subscribe[report_alert]]: {"report_alert":{"0":11}}}[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mCurrent State: ACTIVE[0m
[0m[[0m[31merror[0m] [0m[0mThread State: RUNNABLE[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mLogical Plan:[0m
[0m[[0m[31merror[0m] [0m[0mProject [data#23.harmonywatcher_id AS harmonywatcher_id#25, data#23.current_location AS current_location#26, data#23.surrounding_citizens AS surrounding_citizens#27, data#23.words_heard AS words_heard#28, data#23.alert AS alert#29, data#23.timestamp AS timestamp#30][0m
[0m[[0m[31merror[0m] [0m[0m+- Project [from_json(StructField(harmonywatcher_id,StringType,false), StructField(current_location,StructType(StructField(latitude,DoubleType,false),StructField(longitude,DoubleType,false)),false), StructField(surrounding_citizens,ArrayType(StructType(StructField(name,StringType,false),StructField(harmonyscore,DoubleType,false)),true),false), StructField(words_heard,StringType,false), StructField(alert,BooleanType,false), StructField(timestamp,TimestampType,false), json#21, Some(Europe/Paris)) AS data#23][0m
[0m[[0m[31merror[0m] [0m[0m   +- Project [cast(value#8 as string) AS json#21][0m
[0m[[0m[31merror[0m] [0m[0m      +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@453c3da4, KafkaV2[Subscribe[report_alert]][0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:330)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: org.apache.spark.sql.AnalysisException: Column "surrounding_citizens_temp" not found in schema Some(StructType(StructField(id,IntegerType,false),StructField(harmonywatcher_id,StringType,false),StructField(current_location_latitude,DoubleType,false),StructField(current_location_longitude,DoubleType,false),StructField(words_heard,StringType,true),StructField(surrounding_citizens,StringType,true),StructField(alert,BooleanType,false),StructField(timestamp,TimestampType,false)))[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnNotFoundInSchemaError(QueryCompilationErrors.scala:1192)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:126)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:201)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:126)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.ArrayOps$.map$extension(ArrayOps.scala:934)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:124)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:860)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)[0m
[0m[[0m[31merror[0m] [0m[0m	at consumer_alert$.$anonfun$main$1(consumer_alert.scala:61)[0m
[0m[[0m[31merror[0m] [0m[0m	at consumer_alert$.$anonfun$main$1$adapted(consumer_alert.scala:42)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:660)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:658)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:658)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:255)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrun[0m) org.apache.spark.sql.streaming.StreamingQueryException: Column "surrounding_citizens_temp" not found in schema Some(StructType(StructField(id,IntegerType,false),StructField(harmonywatcher_id,StringType,false),StructField(current_location_latitude,DoubleType,false),StructField(current_location_longitude,DoubleType,false),StructField(words_heard,StringType,true),StructField(surrounding_citizens,StringType,true),StructField(alert,BooleanType,false),StructField(timestamp,TimestampType,false)))[0m
[0m[[0m[31merror[0m] [0m[0m=== Streaming Query ===[0m
[0m[[0m[31merror[0m] [0m[0mIdentifier: [id = 21651d0e-fe6c-489b-bd8b-d7941217915f, runId = 3ab10723-ba20-430b-a49a-2d91588b76f3][0m
[0m[[0m[31merror[0m] [0m[0mCurrent Committed Offsets: {}[0m
[0m[[0m[31merror[0m] [0m[0mCurrent Available Offsets: {KafkaV2[Subscribe[report_alert]]: {"report_alert":{"0":11}}}[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mCurrent State: ACTIVE[0m
[0m[[0m[31merror[0m] [0m[0mThread State: RUNNABLE[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mLogical Plan:[0m
[0m[[0m[31merror[0m] [0m[0mProject [data#23.harmonywatcher_id AS harmonywatcher_id#25, data#23.current_location AS current_location#26, data#23.surrounding_citizens AS surrounding_citizens#27, data#23.words_heard AS words_heard#28, data#23.alert AS alert#29, data#23.timestamp AS timestamp#30][0m
[0m[[0m[31merror[0m] [0m[0m+- Project [from_json(StructField(harmonywatcher_id,StringType,false), StructField(current_location,StructType(StructField(latitude,DoubleType,false),StructField(longitude,DoubleType,false)),false), StructField(surrounding_citizens,ArrayType(StructType(StructField(name,StringType,false),StructField(harmonyscore,DoubleType,false)),true),false), StructField(words_heard,StringType,false), StructField(alert,BooleanType,false), StructField(timestamp,TimestampType,false), json#21, Some(Europe/Paris)) AS data#23][0m
[0m[[0m[31merror[0m] [0m[0m   +- Project [cast(value#8 as string) AS json#21][0m
[0m[[0m[31merror[0m] [0m[0m      +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@453c3da4, KafkaV2[Subscribe[report_alert]][0m
